{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b6202d",
   "metadata": {},
   "source": [
    "# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ –ø–æ–¥–±–æ—Ä–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –∏–∑ Jupyter\n",
    "\n",
    "–ù–∏–∂–µ ‚Äî —è—á–µ–π–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ:\n",
    "- —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏,\n",
    "- –ø–æ–¥–Ω–∏–º–∞—é—Ç FastAPI-—Å–µ—Ä–≤–∏—Å —Å endpoint `/predict`,\n",
    "- –∑–∞–ø—É—Å–∫–∞—é—Ç Gradio UI –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏,\n",
    "- —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -V\n",
    "pip -V\n",
    "pip install --quiet fastapi==0.115.2 uvicorn==0.30.6 pydantic==2.9.2 httpx==0.27.2 tenacity==9.0.0 orjson==3.10.7 gradio==3.50.2 scikit-learn==1.3.2 pandas==2.2.3 numpy==1.26.4 openpyxl==3.1.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e100a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, time, os, json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "# ---- Minimal services (extractor + matcher + catalog loader) ----\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    name: str\n",
    "    manufacturer: str\n",
    "    article: str\n",
    "\n",
    "class PredictResponse(BaseModel):\n",
    "    analogs: List[str]\n",
    "\n",
    "CRITICAL_PARAMETERS: Dict[str, float] = {\n",
    "    \"–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\": 3.0,\n",
    "    \"—Ç–æ–∫\": 3.0,\n",
    "    \"—á–∞—Å—Ç–æ—Ç–∞\": 1.0,\n",
    "    \"–∫–ª–∞—Å—Å_–∑–∞—â–∏—Ç—ã\": 2.0,\n",
    "}\n",
    "\n",
    "def calculate_match_score(target_char: Dict[str, Any], candidate_char: Dict[str, Any]) -> float:\n",
    "    score = 0.0\n",
    "    total = 0.0\n",
    "    for param, weight in CRITICAL_PARAMETERS.items():\n",
    "        total += weight\n",
    "        if param in target_char and param in candidate_char and str(target_char[param]).strip().lower() == str(candidate_char[param]).strip().lower():\n",
    "            score += weight\n",
    "    return (score / total) if total > 0 else 0.0\n",
    "\n",
    "# Very simple in-notebook catalog\n",
    "CATALOG: List[Dict[str, Any]] = [\n",
    "    {\"article\": \"SE-001\", \"characteristics\": {\"–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\": \"220–í\", \"—Ç–æ–∫\": \"10–ê\", \"–∫–ª–∞—Å—Å_–∑–∞—â–∏—Ç—ã\": \"IP20\"}},\n",
    "    {\"article\": \"SE-002\", \"characteristics\": {\"–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\": \"380–í\", \"—Ç–æ–∫\": \"16–ê\", \"–∫–ª–∞—Å—Å_–∑–∞—â–∏—Ç—ã\": \"IP54\"}},\n",
    "    {\"article\": \"SE-003\", \"characteristics\": {\"–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\": \"220–í\", \"—Ç–æ–∫\": \"16–ê\", \"–∫–ª–∞—Å—Å_–∑–∞—â–∏—Ç—ã\": \"IP20\"}},\n",
    "]\n",
    "\n",
    "class CharacteristicExtractor:\n",
    "    def __init__(self):\n",
    "        self.cache: Dict[str, Dict[str, Any]] = {}\n",
    "    def extract(self, product_name: str, manufacturer: str, article: str) -> Dict[str, Any]:\n",
    "        # Minimal heuristic extractor (stub). Real impl would call GigaChat.\n",
    "        # For demo, map common tokens.\n",
    "        text = f\"{product_name} {manufacturer} {article}\".lower()\n",
    "        char = {}\n",
    "        if \"220\" in text:\n",
    "            char[\"–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\"] = \"220–í\"\n",
    "        if \"380\" in text:\n",
    "            char[\"–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\"] = \"380–í\"\n",
    "        if \"10a\" in text or \"10–∞\" in text:\n",
    "            char[\"—Ç–æ–∫\"] = \"10–ê\"\n",
    "        if \"16a\" in text or \"16–∞\" in text:\n",
    "            char[\"—Ç–æ–∫\"] = \"16–ê\"\n",
    "        if \"ip54\" in text:\n",
    "            char[\"–∫–ª–∞—Å—Å_–∑–∞—â–∏—Ç—ã\"] = \"IP54\"\n",
    "        if \"ip20\" in text:\n",
    "            char[\"–∫–ª–∞—Å—Å_–∑–∞—â–∏—Ç—ã\"] = \"IP20\"\n",
    "        return char\n",
    "\n",
    "extractor = CharacteristicExtractor()\n",
    "\n",
    "app = FastAPI(title=\"Notebook Analog Matcher\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictResponse)\n",
    "def predict(req: PredictRequest) -> PredictResponse:\n",
    "    if not req.name or not req.manufacturer or not req.article:\n",
    "        raise HTTPException(status_code=400, detail=\"Missing required fields\")\n",
    "    target_char = extractor.extract(req.name, req.manufacturer, req.article)\n",
    "    scored = []\n",
    "    for p in CATALOG:\n",
    "        sc = calculate_match_score(target_char, p.get(\"characteristics\", {}))\n",
    "        scored.append({**p, \"score\": sc})\n",
    "    scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    top = [p[\"article\"] for p in scored[:6]]\n",
    "    return PredictResponse(analogs=top)\n",
    "\n",
    "# ---- Background server thread ----\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
    "\n",
    "if \"_server_thread\" not in globals():\n",
    "    _server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "    _server_thread.start()\n",
    "    time.sleep(1)\n",
    "print(\"FastAPI server started on http://localhost:8000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9df70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/predict\"\n",
    "payload = {\n",
    "    \"name\": \"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–∫–ª—é—á–∞—Ç–µ–ª—å 220V 16A IP20\",\n",
    "    \"manufacturer\": \"EKF\",\n",
    "    \"article\": \"A1234\"\n",
    "}\n",
    "\n",
    "r = requests.post(url, json=payload, timeout=10)\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0043492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def client_predict(name, manufacturer, article):\n",
    "    import requests\n",
    "    url = \"http://localhost:8000/predict\"\n",
    "    payload = {\"name\": name, \"manufacturer\": manufacturer, \"article\": article}\n",
    "    try:\n",
    "        r = requests.post(url, json=payload, timeout=10)\n",
    "        data = r.json()\n",
    "        return \"\\n\".join(data.get(\"analogs\", []))\n",
    "    except Exception as e:\n",
    "        return f\"–û—à–∏–±–∫–∞: {e}\"\n",
    "\n",
    "with gr.Blocks(title=\"Analog Matcher ‚Äî Notebook UI\") as demo:\n",
    "    gr.Markdown(\"### –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π API\")\n",
    "    with gr.Row():\n",
    "        name = gr.Textbox(label=\"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ\", value=\"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–∫–ª—é—á–∞—Ç–µ–ª—å 220V 16A IP20\")\n",
    "        manufacturer = gr.Textbox(label=\"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å\", value=\"EKF\")\n",
    "        article = gr.Textbox(label=\"–ê—Ä—Ç–∏–∫—É–ª\", value=\"A1234\")\n",
    "    btn = gr.Button(\"–í—ã–ø–æ–ª–Ω–∏—Ç—å /predict\")\n",
    "    out = gr.Textbox(label=\"–û—Ç–≤–µ—Ç (–∞—Ä—Ç–∏–∫—É–ª—ã)\")\n",
    "    btn.click(client_predict, inputs=[name, manufacturer, article], outputs=out)\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea193b0-8e85-4121-8226-ccfb50e393b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61168075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf10199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50b254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed483ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df540851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c236c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffaf727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image_index = faiss.read_index(\"../create_index/image_index.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index.faiss\")\n",
    "\n",
    "def generate_caption(image: Image.Image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def get_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_clip_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    features = features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(features)\n",
    "    return features\n",
    "\n",
    "def get_results_with_images(embedding, index, top_k=2):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def search_similar_images(image: Image.Image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    text_emb = get_clip_text_embedding(caption)\n",
    "    image_emb = get_clip_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return caption, text_results, image_results\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=search_similar_images,\n",
    "    inputs=gr.Image(label=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\"),\n",
    "        gr.Gallery(label=\"üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (CLIP)\", height=\"auto\", columns=2),\n",
    "        gr.Gallery(label=\"üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (CLIP)\", height=\"auto\", columns=2)\n",
    "    ],\n",
    "    title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\",\n",
    "    description=\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å BLIP —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –∞ CLIP –Ω–∞–π–¥—ë—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e20c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef1820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8940a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09062cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a91000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865caaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ffd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e10cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ba07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86d33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5276b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a25e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33025167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e1628-2b3d-468f-8f4c-890d9027f8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
